[
    "distil-whisper: robust knowledge distillation via large-scale pseudo labelling",
    "multimodal hate speech detection via multi-scale visual kernels and knowledge distillation architecture",
    "radardistill: boosting radar-based object detection performance via knowledge distillation from lidar features",
    "semantic-aware knowledge distillation with parameter-free feature uniformization",
    "frequency attention for knowledge distillation",
    "kd-dlgan: data limited image generation via knowledge distillation",
    "scl-ikd: intermediate knowledge distillation via supervised contrastive representation learning",
    "implicit chain of thought reasoning via knowledge distillation",
    "knowledge distillation based on transformed teacher matching",
    "boosting graph neural networks via adaptive knowledge distillation",
    "online knowledge distillation-based multiscale threshold denoising networks for fault diagnosis of transmission systems",
    "knowledge distillation model for acute lymphoblastic leukemia detection: exploring the impact of nesterov-accelerated adaptive moment estimation optimizer",
    "efficient knowledge distillation for remote sensing image classification: a cnn-based approach",
    "unbiased knowledge distillation for recommendation",
    "automated knowledge distillation via monte carlo tree search",
    "cross-domain visual prompting with spatial proximity knowledge distillation for histological image classification",
    "rethinking kullback-leibler divergence in knowledge distillation for large language models",
    "learning to retain while acquiring: combating distribution-shift in adversarial data-free knowledge distillation",
    "dafkd: domain-aware federated knowledge distillation",
    "the best of both worlds: accurate global and personalized models through federated learning with data-free hyper-knowledge distillation",
    "crosskd: cross-head knowledge distillation for object detection",
    "teacher-student collaborative knowledge distillation for image classification",
    "parameter-efficient and student-friendly knowledge distillation",
    "reciprocal teacher-student learning via forward and feedback knowledge distillation",
    "a comprehensive survey on knowledge distillation of diffusion models",
    "melanoma classification from dermatoscopy images using knowledge distillation for highly imbalanced data",
    "reducing llm hallucination using knowledge distillation: a case study with mistral large and mmlu benchmark",
    "siamohot: a lightweight dual siamese network for onboard hyperspectral object tracking via joint spatial-spectral knowledge distillation",
    "graph attention guidance network with knowledge distillation for semantic segmentation of remote sensing images",
    "multi-granularity knowledge distillation and prototype consistency regularization for class-incremental learning",
    "reconstructed graph neural network with knowledge distillation for lightweight anomaly detection",
    "fedgkd: toward heterogeneous federated learning via global knowledge distillation",
    "bge m3-embedding: multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
    "itkd: interchange transfer-based knowledge distillation for 3d object detection",
    "panda: prompt transfer meets knowledge distillation for efficient model adaptation",
    "structured knowledge distillation for accurate and efficient object detection",
    "remembering normality: memory-guided knowledge distillation for unsupervised anomaly detection",
    "what knowledge gets distilled in knowledge distillation?",
    "low-light image enhancement with knowledge distillation",
    "a novel tensor decomposition-based efficient detector for low-altitude aerial objects with knowledge distillation scheme",
    "m3-embedding: multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
    "kd-par: a knowledge distillation-based pedestrian attribute recognition model with multi-label mixed feature learning network",
    "emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals",
    "learning from human educational wisdom: a student-centered knowledge distillation method",
    "learnable cross-modal knowledge distillation for multi-modal learning with missing modality",
    "xkd: cross-modal knowledge distillation with domain alignment for video representation learning",
    "boosting lightweight depth estimation via knowledge distillation",
    "understanding the role of the projector in knowledge distillation",
    "compact language models via pruning and knowledge distillation",
    "balanced knowledge distillation for long-tailed learning",
    "cekd: cross-modal edge-privileged knowledge distillation for semantic scene understanding using only thermal images",
    "knowledge distillation guided interpretable brain subgraph neural networks for brain disorder exploration",
    "using multimodal contrastive knowledge distillation for video-text retrieval",
    "when object detection meets knowledge distillation: a survey",
    "consistency-and dependence-guided knowledge distillation for object detection in remote sensing images",
    "a lightweight crack segmentation network based on knowledge distillation",
    "empowering in-network classification in programmable switches by binary decision tree and knowledge distillation",
    "logit standardization in knowledge distillation",
    "kd-lightnet: a lightweight network based on knowledge distillation for industrial defect detection",
    "metafed: federated learning among federations with cyclic knowledge distillation for personalized healthcare",
    "large language model guided knowledge distillation for time series anomaly detection",
    "dual teachers for self-knowledge distillation",
    "reliant: fair knowledge distillation for graph neural networks",
    "ensemble modeling with contrastive knowledge distillation for sequential recommendation",
    "dual learning with dynamic knowledge distillation for partially relevant video retrieval",
    "from knowledge distillation to self-knowledge distillation: a unified approach with normalized loss and customized soft labels",
    "contrastive learning-based knowledge distillation for rgb-thermal urban scene semantic segmentation",
    "multi-target knowledge distillation via student self-reflection",
    "transkd: transformer knowledge distillation for efficient semantic segmentation",
    "a survey on knowledge distillation of large language models",
    "multi-mode online knowledge distillation for self-supervised visual representation learning",
    "constructing deep spiking neural networks from artificial neural networks with knowledge distillation",
    "knowledge distillation-based domain-invariant representation learning for domain generalization",
    "feature alignment-based knowledge distillation for efficient compression of large language models",
    "end-to-end zero-shot hoi detection via vision and language knowledge distillation",
    "rethinking feature-based knowledge distillation for face recognition",
    "norm: knowledge distillation via n-to-one representation matching",
    "dynamic data-free knowledge distillation by easy-to-hard learning strategy",
    "unidistill: a universal cross-modality knowledge distillation framework for 3d object detection in bird's-eye view",
    "prototype knowledge distillation for medical segmentation with missing modality",
    "hierarchical multi-attention transfer for knowledge distillation",
    "improving knowledge distillation via regularizing feature norm and direction",
    "gkd: generalized knowledge distillation for auto-regressive sequence models",
    "a light-weight object detection method based on knowledge distillation and model pruning for seam tracking system",
    "baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty",
    "knowledge distillation on graphs: a survey",
    "one-for-all: bridge the gap between heterogeneous architectures in knowledge distillation",
    "an explainable knowledge distillation method with xgboost for icu mortality prediction",
    "speculative knowledge distillation: bridging the teacher-student gap through interleaved sampling",
    "data-free knowledge distillation via feature exchange and activation region constraint",
    "supervised masked knowledge distillation for few-shot transformers",
    "knowledge distillation with segment anything (sam) model for planetary geological mapping",
    "multitask knowledge distillation guides end-to-end lane detection",
    "c2kd: bridging the modality gap for cross-modal knowledge distillation",
    "language-oriented communication with semantic coding and knowledge distillation for text-to-image generation",
    "ckdh: clip-based knowledge distillation hashing for cross-modal retrieval",
    "knowledge distillation facilitates the lightweight and efficient plant diseases detection model",
    "mcc-kd: multi-cot consistent knowledge distillation",
    "two-stage reverse knowledge distillation incorporated and self-supervised masking strategy for industrial anomaly detection",
    "survey on knowledge distillation for large language models: methods, evaluation, and application",
    "student-friendly knowledge distillation",
    "nayer: noisy layer data generation for efficient and effective data-free knowledge distillation",
    "detrdistill: a universal knowledge distillation framework for detr-families",
    "local and global knowledge distillation with direction-enhanced contrastive learning for single-image deraining",
    "fedbikd: federated bidirectional knowledge distillation for distracted driving detection",
    "online knowledge distillation via mutual contrastive learning for visual recognition",
    "vision transformers for small histological datasets learned through knowledge distillation",
    "digital twin enhanced federated reinforcement learning with lightweight knowledge distillation in mobile networks",
    "knowledge distillation in vision transformers: a critical review",
    "higher performance of mistral large on mmlu benchmark through two-stage knowledge distillation",
    "distillspec: improving speculative decoding via knowledge distillation",
    "wavenet: wavelet network with knowledge distillation for rgb-t salient object detection",
    "progressive knowledge distillation of stable diffusion xl using layer level loss",
    "incrementer: transformer for class-incremental semantic segmentation with knowledge distillation focusing on old class",
    "x3kd: knowledge distillation across modalities, tasks and stages for multi-camera 3d object detection",
    "efficient model compression and knowledge distillation on llama 2: achieving high performance with reduced computational cost",
    "categories of response-based, feature-based, and relation-based knowledge distillation",
    "boosting knowledge distillation via intra-class logit distribution smoothing",
    "a lightweight network for photovoltaic cell defect detection in electroluminescence images based on neural architecture search and knowledge distillation",
    "minillm: knowledge distillation of large language models",
    "tasked: transformer-based adversarial learning for human activity recognition using wearable sensors via self-knowledge distillation",
    "semi-supervised semantic segmentation with mutual knowledge distillation",
    "normkd: normalized logits for knowledge distillation",
    "generative adversarial super-resolution at the edge with knowledge distillation",
    "knowledge distillation in automated annotation: supervised text classification with llm-generated training labels",
    "self-knowledge distillation via dropout",
    "curriculum temperature for knowledge distillation",
    "disentangle and remerge: interventional knowledge distillation for few-shot object detection from a conditional causal perspective",
    "multiple source-free domain adaptation network based on knowledge distillation for machinery fault diagnosis",
    "ssd-kd: a self-supervised diverse knowledge distillation method for lightweight skin lesion classification using dermoscopic images",
    "communication-efficient federated learning on non-iid data using two-step knowledge distillation",
    "few-shot radar jamming recognition network via time-frequency self-attention and global knowledge distillation",
    "dgpinet-kd: deep guided and progressive integration network with knowledge distillation for rgb-d indoor scene analysis",
    "pointdistiller: structured knowledge distillation towards efficient and compact 3d detection",
    "class attention transfer based knowledge distillation",
    "one-class knowledge distillation for spoofing speech detection",
    "freekd: knowledge distillation via semantic frequency prompt",
    "knowledge distillation of llm for education",
    "segmentation with mixed supervision: confidence maximization helps knowledge distillation",
    "an adaptive teacher–student learning algorithm with decomposed knowledge distillation for on-edge intelligence",
    "distillbev: boosting multi-camera 3d object detection with cross-modal knowledge distillation",
    "mobilevos: real-time video object segmentation contrastive learning meets knowledge distillation",
    "pa-seg: learning from point annotations for 3d medical image segmentation using contextual regularization and cross knowledge distillation",
    "vanillakd: revisit the power of vanilla knowledge distillation from small scale to large scale",
    "fedack: federated adversarial contrastive knowledge distillation for cross-lingual and cross-model social bot detection",
    "label semantic knowledge distillation for unbiased scene graph generation",
    "mkd-cooper: cooperative 3d object detection for autonomous driving via multi-teacher knowledge distillation",
    "adaptive multi-teacher knowledge distillation with meta-learning",
    "collaborative multi-teacher knowledge distillation for learning low bit-width deep neural networks",
    "efficient large-scale audio tagging via transformer-to-cnn knowledge distillation"
]