#!/usr/bin/env python3
"""
çœŸå®é‚®ç®±æŸ¥æ‰¾å™¨
ä»Google Scholarä¸ªäººä¸»é¡µæå–ä¸ªäººç½‘ç«™é“¾æ¥ï¼Œç„¶åä»ä¸ªäººç½‘ç«™æå–é‚®ç®±
"""
import asyncio
import aiohttp
import re
from typing import List, Optional
from bs4 import BeautifulSoup
import urllib.parse


class RealEmailFinder:
    """çœŸå®é‚®ç®±æŸ¥æ‰¾å™¨"""
    
    def __init__(self, proxy: Optional[str] = None):
        """
        åˆå§‹åŒ–é‚®ç®±æŸ¥æ‰¾å™¨
        
        Args:
            proxy: ä»£ç†è®¾ç½®ï¼Œä¾‹å¦‚ "http://127.0.0.1:7890"
        """
        self.proxy = proxy
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector()

        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=60)  # å¢åŠ è¶…æ—¶æ—¶é—´
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def _get_personal_website_from_scholar_profile(self, scholar_url: str) -> Optional[str]:
        """
        ä»Google Scholarä¸ªäººä¸»é¡µæå–ä¸ªäººç½‘ç«™é“¾æ¥

        Args:
            scholar_url: Google Scholarä¸ªäººä¸»é¡µURL

        Returns:
            ä¸ªäººç½‘ç«™URLæˆ–None
        """
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®Google Scholarä¸ªäººä¸»é¡µ: {scholar_url}")

            # è®¾ç½®ä»£ç†
            proxy_url = self.proxy if self.proxy else None

            async with self.session.get(scholar_url, proxy=proxy_url) as response:
                if response.status != 200:
                    print(f"âŒ è®¿é—®Google Scholarå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return None

                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                # æŸ¥æ‰¾ä¸ªäººä¸»é¡µé“¾æ¥
                homepage_links = []

                print(f"ğŸ” å¼€å§‹è§£æGoogle Scholarä¸ªäººä¸»é¡µæºä»£ç ...")

                # æ–¹æ³•1: ç›´æ¥åœ¨é¡µé¢æºä»£ç ä¸­æœç´¢ github.io é“¾æ¥
                print(f"ğŸ“ æ–¹æ³•1: åœ¨é¡µé¢æºä»£ç ä¸­æœç´¢ github.io é“¾æ¥...")
                import re

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åœ¨æ•´ä¸ªHTMLæºä»£ç ä¸­æŸ¥æ‰¾github.ioé“¾æ¥
                github_io_pattern = r'https?://[a-zA-Z0-9\-_.]+\.github\.io[/]?[^"\s<>]*'
                github_links = re.findall(github_io_pattern, html)

                for link in github_links:
                    # æ¸…ç†é“¾æ¥ï¼Œç§»é™¤å¯èƒ½çš„å°¾éƒ¨å­—ç¬¦
                    clean_link = link.rstrip('",;')
                    if self._is_external_personal_website(clean_link):
                        homepage_links.append(clean_link)
                        print(f"âœ… ä»æºä»£ç æ­£åˆ™åŒ¹é…æ‰¾åˆ°GitHub Pages: {clean_link}")

                # æ–¹æ³•2: æŸ¥æ‰¾ä¸ªäººä¿¡æ¯åŒºåŸŸçš„é“¾æ¥
                print(f"ğŸ“ æ–¹æ³•2: æŸ¥æ‰¾ä¸ªäººä¿¡æ¯åŒºåŸŸçš„é“¾æ¥...")
                info_sections = [
                    soup.find('div', id='gsc_prf_i'),  # ä¸ªäººä¿¡æ¯ä¸»åŒºåŸŸ
                    soup.find('div', id='gsc_prf_ivh'),  # ä¸ªäººä¿¡æ¯éªŒè¯åŒºåŸŸ
                    soup.find('div', class_='gsc_prf_il'),  # ä¸ªäººä¿¡æ¯åˆ—è¡¨
                ]

                for section in info_sections:
                    if section:
                        for link in section.find_all('a', href=True):
                            href = link.get('href')
                            if href and 'github.io' in href.lower():
                                if self._is_external_personal_website(href):
                                    homepage_links.append(href)
                                    print(f"âœ… ä»ä¸ªäººä¿¡æ¯åŒºåŸŸæ‰¾åˆ°GitHub Pages: {href}")

                # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«github.ioçš„é“¾æ¥
                print(f"ğŸ“ æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰åŒ…å«github.ioçš„é“¾æ¥...")
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href')
                    if href and 'github.io' in href.lower():
                        if self._is_external_personal_website(href):
                            homepage_links.append(href)
                            link_text = link.get_text().strip()
                            print(f"âœ… ä»HTMLé“¾æ¥æ‰¾åˆ°GitHub Pages: {href} (æ–‡æœ¬: {link_text})")

                # æ–¹æ³•4: ä½¿ç”¨CSSé€‰æ‹©å™¨ä¸“é—¨æŸ¥æ‰¾github.ioé“¾æ¥
                print(f"ğŸ“ æ–¹æ³•4: ä½¿ç”¨CSSé€‰æ‹©å™¨æŸ¥æ‰¾github.ioé“¾æ¥...")
                css_selectors = [
                    'a[href*="github.io"]',  # ç›´æ¥æŸ¥æ‰¾åŒ…å«github.ioçš„é“¾æ¥
                    'a.gsc_prf_ila[href*="github.io"]',  # ä¸ªäººä¿¡æ¯åŒºåŸŸçš„github.ioé“¾æ¥
                ]

                for selector in css_selectors:
                    try:
                        links = soup.select(selector)
                        for link in links:
                            href = link.get('href')
                            if href and self._is_external_personal_website(href):
                                homepage_links.append(href)
                                print(f"âœ… ä»CSSé€‰æ‹©å™¨ {selector} æ‰¾åˆ°GitHub Pages: {href}")
                    except Exception as e:
                        print(f"âš ï¸ CSSé€‰æ‹©å™¨ {selector} è§£æå¤±è´¥: {e}")

                # æ–¹æ³•5: åœ¨é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾å¯èƒ½çš„github.ioé“¾æ¥
                print(f"ğŸ“ æ–¹æ³•5: åœ¨é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾github.ioé“¾æ¥...")
                page_text = soup.get_text()
                text_github_links = re.findall(github_io_pattern, page_text)
                for link in text_github_links:
                    clean_link = link.rstrip('",;')
                    if self._is_external_personal_website(clean_link):
                        homepage_links.append(clean_link)
                        print(f"âœ… ä»é¡µé¢æ–‡æœ¬æ‰¾åˆ°GitHub Pages: {clean_link}")

                # å»é‡
                homepage_links = list(set(homepage_links))

                if homepage_links:
                    print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(homepage_links)} ä¸ªå€™é€‰ä¸ªäººä¸»é¡µé“¾æ¥")

                    # ä¸“é—¨æŸ¥æ‰¾ä»¥ github.io/ ç»“å°¾çš„ä¸ªäººä¸»é¡µ
                    github_pages_links = []
                    for link in homepage_links:
                        # æ£€æŸ¥æ˜¯å¦ä»¥ github.io/ ç»“å°¾ï¼ˆå…è®¸æœ‰æˆ–æ²¡æœ‰å°¾éƒ¨æ–œæ ï¼‰
                        if link.lower().rstrip('/').endswith('github.io'):
                            github_pages_links.append(link)
                            print(f"ğŸ¯ æ‰¾åˆ°GitHub Pagesä¸ªäººä¸»é¡µ: {link}")

                    if github_pages_links:
                        # å¦‚æœæ‰¾åˆ°å¤šä¸ªGitHub Pagesé“¾æ¥ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                        selected_link = github_pages_links[0]
                        print(f"âœ… é€‰æ‹©GitHub Pagesä¸ªäººä¸»é¡µ: {selected_link}")
                        return selected_link
                    else:
                        print(f"âš ï¸ æœªæ‰¾åˆ°ä»¥ github.io/ ç»“å°¾çš„ä¸ªäººä¸»é¡µ")
                        print(f"ğŸ“‹ æ‰¾åˆ°çš„é“¾æ¥:")
                        for i, link in enumerate(homepage_links):
                            print(f"   {i+1}. {link}")
                        print(f"ğŸ’¡ åªæ¥å—ä»¥ github.io/ ç»“å°¾çš„ä¸ªäººä¸»é¡µ")
                        return None

                print(f"âš ï¸ æœªåœ¨Google Scholarä¸ªäººä¸»é¡µä¸­æ‰¾åˆ°ä¸ªäººç½‘ç«™é“¾æ¥")
                return None

        except Exception as e:
            print(f"âŒ æå–ä¸ªäººç½‘ç«™é“¾æ¥æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _is_external_personal_website(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯å¤–éƒ¨ä¸ªäººç½‘ç«™é“¾æ¥ï¼ˆä¸“é—¨æŸ¥æ‰¾ GitHub Pagesï¼‰

        Args:
            url: è¦æ£€æŸ¥çš„URL

        Returns:
            æ˜¯å¦æ˜¯ GitHub Pages ä¸ªäººç½‘ç«™
        """
        if not url:
            return False

        # æ’é™¤Google Scholarå†…éƒ¨é“¾æ¥
        if 'scholar.google.com' in url:
            return False

        # æ’é™¤å…¶ä»–GoogleæœåŠ¡
        google_domains = ['google.com', 'gmail.com', 'googleusercontent.com', 'gstatic.com']
        if any(domain in url.lower() for domain in google_domains):
            return False

        # æ’é™¤æ˜æ˜¾çš„éä¸ªäººç½‘ç«™
        excluded_patterns = [
            'javascript:', 'mailto:', '#',
            'facebook.com', 'twitter.com', 'linkedin.com',
            'researchgate.net', 'orcid.org'
        ]
        if any(pattern in url.lower() for pattern in excluded_patterns):
            return False

        # å¿…é¡»æ˜¯HTTP/HTTPSé“¾æ¥
        if not url.startswith(('http://', 'https://')):
            return False

        # ä¸“é—¨æŸ¥æ‰¾ GitHub Pages (ä»¥ github.io ç»“å°¾)
        if url.lower().rstrip('/').endswith('github.io'):
            return True

        # ä¸æ¥å—å…¶ä»–ç±»å‹çš„ä¸ªäººç½‘ç«™
        return False
    
    async def _extract_emails_from_website(self, website_url: str) -> List[str]:
        """
        ä»ä¸ªäººç½‘ç«™æå–é‚®ç®±ï¼Œä¼˜å…ˆæŸ¥æ‰¾ mailto: é“¾æ¥

        Args:
            website_url: ä¸ªäººç½‘ç«™URL

        Returns:
            é‚®ç®±åˆ—è¡¨
        """
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®ä¸ªäººç½‘ç«™: {website_url}")

            # è®¾ç½®ä»£ç†
            proxy_url = self.proxy if self.proxy else None

            async with self.session.get(website_url, proxy=proxy_url) as response:
                if response.status != 200:
                    print(f"âŒ è®¿é—®ä¸ªäººç½‘ç«™å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                    return []

                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                emails = set()

                print(f"ğŸ” å¼€å§‹ä»ä¸ªäººç½‘ç«™æå–é‚®ç®±...")

                # åªæŸ¥æ‰¾ mailto: é“¾æ¥ï¼ˆæœ€å¯é ä¸”çœŸå®çš„æ–¹æ³•ï¼‰
                print(f"ğŸ“§ æŸ¥æ‰¾ mailto: é“¾æ¥...")
                mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.I))

                if mailto_links:
                    for i, link in enumerate(mailto_links):
                        href = link.get('href')
                        if href:
                            # æå–é‚®ç®±åœ°å€ï¼Œå¤„ç† mailto:email@domain.com?subject=... æ ¼å¼
                            email_part = href.replace('mailto:', '').split('?')[0].split('&')[0].strip()
                            if self._is_valid_email(email_part) and not self._is_spam_email(email_part):
                                emails.add(email_part)
                                print(f"âœ… ä» mailto: é“¾æ¥æ‰¾åˆ°é‚®ç®± {i+1}: {email_part}")
                                print(f"   å®Œæ•´é“¾æ¥: {href}")
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• mailto: é“¾æ¥")

                # åœ¨ç‰¹å®šå…ƒç´ ä¸­æŸ¥æ‰¾ mailto: é“¾æ¥
                print(f"ğŸ“§ åœ¨ç‰¹å®šå…ƒç´ ä¸­æŸ¥æ‰¾ mailto: é“¾æ¥...")
                contact_selectors = [
                    '.contact', '.email', '#contact', '#email',
                    '.contact-info', '.contact-email', '.author-email',
                    '[class*="contact"]', '[class*="email"]', '[class*="mail"]',
                    '[id*="contact"]', '[id*="email"]', '[id*="mail"]',
                    'footer', '.footer', '#footer',
                    '.about', '#about', '.bio', '#bio'
                ]

                for selector in contact_selectors:
                    try:
                        elements = soup.select(selector)
                        for element in elements:
                            # åªæ£€æŸ¥å…ƒç´ ä¸­çš„ mailto é“¾æ¥
                            element_mailto_links = element.find_all('a', href=re.compile(r'^mailto:', re.I))
                            for link in element_mailto_links:
                                href = link.get('href')
                                if href:
                                    email_part = href.replace('mailto:', '').split('?')[0].split('&')[0].strip()
                                    if self._is_valid_email(email_part) and not self._is_spam_email(email_part):
                                        emails.add(email_part)
                                        print(f"âœ… ä» {selector} å…ƒç´ çš„ mailto: é“¾æ¥æ‰¾åˆ°é‚®ç®±: {email_part}")
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†é€‰æ‹©å™¨ {selector} æ—¶å‡ºé”™: {e}")
                        continue

                # æ–¹æ³•3: æŸ¥æ‰¾é¡µé¢æ–‡æœ¬ä¸­çš„æ ‡å‡†é‚®ç®±æ ¼å¼ï¼ˆå¦‚ user@mail.domain.comï¼‰
                print(f"ğŸ“§ æ–¹æ³•3: æŸ¥æ‰¾é¡µé¢æ–‡æœ¬ä¸­çš„æ ‡å‡†é‚®ç®±...")
                text_emails = self._find_text_emails(soup)
                for email in text_emails:
                    if self._is_valid_email(email) and not self._is_spam_email(email):
                        emails.add(email)
                        print(f"âœ… ä»é¡µé¢æ–‡æœ¬æ‰¾åˆ°é‚®ç®±: {email}")

                # æ–¹æ³•4: æŸ¥æ‰¾æ··æ·†æ ¼å¼çš„é‚®ç®±ï¼ˆå¦‚ "user AT domain DOT com"ï¼‰
                print(f"ğŸ“§ æ–¹æ³•4: æŸ¥æ‰¾æ··æ·†æ ¼å¼çš„é‚®ç®±...")
                obfuscated_emails = self._find_obfuscated_emails(soup)
                for email in obfuscated_emails:
                    if self._is_valid_email(email) and not self._is_spam_email(email):
                        emails.add(email)
                        print(f"âœ… ä»æ··æ·†æ ¼å¼æ‰¾åˆ°é‚®ç®±: {email}")

                # æ–¹æ³•5: æŸ¥æ‰¾åˆå¹¶æ ¼å¼çš„é‚®ç®±ï¼ˆå¦‚ {user1,user2}@domain.comï¼‰
                print(f"ğŸ“§ æ–¹æ³•5: æŸ¥æ‰¾åˆå¹¶æ ¼å¼çš„é‚®ç®±...")
                merged_emails = self._find_merged_emails(soup)
                for email in merged_emails:
                    if self._is_valid_email(email) and not self._is_spam_email(email):
                        emails.add(email)
                        print(f"âœ… ä»åˆå¹¶æ ¼å¼æ‰¾åˆ°é‚®ç®±: {email}")

                email_list = list(emails)
                if email_list:
                    print(f"ğŸ‰ æ€»å…±æ‰¾åˆ° {len(email_list)} ä¸ªçœŸå®é‚®ç®± (ä»…æ¥è‡ª mailto: é“¾æ¥):")
                    for i, email in enumerate(email_list):
                        print(f"   {i+1}. {email}")
                else:
                    print(f"âš ï¸ æœªåœ¨ä¸ªäººç½‘ç«™ä¸­æ‰¾åˆ°ä»»ä½• mailto: é‚®ç®±é“¾æ¥")

                return email_list

        except Exception as e:
            print(f"âŒ ä»ä¸ªäººç½‘ç«™æå–é‚®ç®±æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _is_valid_email(self, email: str) -> bool:
        """éªŒè¯é‚®ç®±æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        if not email or len(email) > 254:
            return False
        
        # åŸºæœ¬çš„é‚®ç®±æ ¼å¼éªŒè¯
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))
    
    def _is_spam_email(self, email: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯åƒåœ¾é‚®ç®±æˆ–æ— æ•ˆé‚®ç®±"""
        email_lower = email.lower()

        # æ˜æ˜¾çš„åƒåœ¾é‚®ç®±åŸŸå
        spam_domains = [
            'example.com', 'test.com', 'dummy.com', 'localhost',
            'tempmail.com', '10minutemail.com', 'guerrillamail.com'
        ]

        # é€šç”¨çš„ç³»ç»Ÿé‚®ç®±å‰ç¼€
        spam_prefixes = [
            'noreply', 'no-reply', 'donotreply', 'do-not-reply',
            'admin', 'webmaster', 'info', 'support', 'contact',
            'hello', 'help', 'service', 'sales', 'marketing',
            'postmaster', 'mailer-daemon', 'root', 'daemon'
        ]

        # æ£€æŸ¥åŸŸå
        for domain in spam_domains:
            if domain in email_lower:
                return True

        # æ£€æŸ¥é‚®ç®±å‰ç¼€ï¼ˆ@ç¬¦å·å‰çš„éƒ¨åˆ†ï¼‰
        email_prefix = email_lower.split('@')[0] if '@' in email_lower else email_lower
        for prefix in spam_prefixes:
            if email_prefix == prefix:  # å®Œå…¨åŒ¹é…
                return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æµ‹è¯•å­—ç¬¦ä¸²
        test_patterns = ['test', 'demo', 'sample', 'fake', 'invalid']
        for pattern in test_patterns:
            if pattern in email_prefix:
                return True

        return False

    def _find_text_emails(self, soup: BeautifulSoup) -> List[str]:
        """
        æŸ¥æ‰¾é¡µé¢æ–‡æœ¬ä¸­çš„æ ‡å‡†é‚®ç®±æ ¼å¼

        æ”¯æŒçš„æ ¼å¼ï¼š
        - user@mail.domain.com
        - researcher@cs.university.edu
        - admin@dept.nankai.edu.cn

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            æ‰¾åˆ°çš„é‚®ç®±åˆ—è¡¨
        """
        emails = []

        try:
            # è·å–é¡µé¢æ–‡æœ¬
            page_text = soup.get_text()

            # æ ‡å‡†é‚®ç®±çš„æ­£åˆ™è¡¨è¾¾å¼
            # åŒ¹é…æ ‡å‡†çš„é‚®ç®±æ ¼å¼ï¼Œä½†æ’é™¤å·²ç»åœ¨mailtoé“¾æ¥ä¸­çš„
            email_pattern = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'

            print(f"ğŸ” åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢æ ‡å‡†é‚®ç®±æ ¼å¼...")

            # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„é‚®ç®±
            matches = re.finditer(email_pattern, page_text, re.IGNORECASE)

            for match in matches:
                email = match.group(0).strip()

                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨mailtoé“¾æ¥ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                if not self._is_email_in_mailto_links(soup, email):
                    emails.append(email)
                    print(f"âœ… æ‰¾åˆ°æ–‡æœ¬é‚®ç®±: {email}")
                else:
                    print(f"âš ï¸ é‚®ç®±å·²åœ¨mailtoé“¾æ¥ä¸­ï¼Œè·³è¿‡: {email}")

            # å»é‡
            unique_emails = list(set(emails))

            if unique_emails:
                print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(unique_emails)} ä¸ªæ–‡æœ¬é‚®ç®±")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬é‚®ç®±")

            return unique_emails

        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾æ–‡æœ¬é‚®ç®±æ—¶å‡ºé”™: {e}")
            return []

    def _is_email_in_mailto_links(self, soup: BeautifulSoup, email: str) -> bool:
        """
        æ£€æŸ¥é‚®ç®±æ˜¯å¦å·²ç»åœ¨mailtoé“¾æ¥ä¸­

        Args:
            soup: BeautifulSoupå¯¹è±¡
            email: è¦æ£€æŸ¥çš„é‚®ç®±

        Returns:
            å¦‚æœé‚®ç®±å·²åœ¨mailtoé“¾æ¥ä¸­è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            import re
            mailto_links = soup.find_all('a', href=re.compile(r'^mailto:', re.I))

            for link in mailto_links:
                href = link.get('href')
                if href:
                    # æå–mailtoé“¾æ¥ä¸­çš„é‚®ç®±
                    email_part = href.replace('mailto:', '').split('?')[0].split('&')[0].strip()
                    if email_part.lower() == email.lower():
                        return True

            return False

        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥mailtoé“¾æ¥æ—¶å‡ºé”™: {e}")
            return False

    def _find_obfuscated_emails(self, soup: BeautifulSoup) -> List[str]:
        """
        æŸ¥æ‰¾æ··æ·†æ ¼å¼çš„é‚®ç®±

        æ”¯æŒçš„æ ¼å¼ï¼š
        - user AT domain DOT com
        - user at domain dot com
        - user [AT] domain [DOT] com
        - user (at) domain (dot) com
        - fushuo.huo AT connect dot polyu dot hk

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            æ‰¾åˆ°çš„é‚®ç®±åˆ—è¡¨
        """
        emails = []

        try:
            # è·å–é¡µé¢æ–‡æœ¬
            page_text = soup.get_text()

            # å®šä¹‰æ··æ·†é‚®ç®±çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            obfuscated_patterns = [
                # åŸºæœ¬çš„ AT/DOT æ ¼å¼ï¼ˆåŸŸåè¢«DOTåˆ†éš”ï¼‰
                r'\b([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+)\s+DOT\s+([a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s+at\s+([a-zA-Z0-9.-]+)\s+dot\s+([a-zA-Z]{2,})\b',

                # å¸¦æ‹¬å·çš„æ ¼å¼ï¼ˆåŸŸåè¢«DOTåˆ†éš”ï¼‰
                r'\b([a-zA-Z0-9._%+-]+)\s*\[AT\]\s*([a-zA-Z0-9.-]+)\s*\[DOT\]\s*([a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s*\(at\)\s*([a-zA-Z0-9.-]+)\s*\(dot\)\s*([a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s*\(AT\)\s*([a-zA-Z0-9.-]+)\s*\(DOT\)\s*([a-zA-Z]{2,})\b',

                # æ›´å¤æ‚çš„æ ¼å¼ï¼Œæ”¯æŒå¤šä¸ªç‚¹ï¼ˆå¦‚ connect dot polyu dot hkï¼‰
                r'\b([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+(?:\s+dot\s+[a-zA-Z0-9.-]+)*)\s+dot\s+([a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s+at\s+([a-zA-Z0-9.-]+(?:\s+dot\s+[a-zA-Z0-9.-]+)*)\s+dot\s+([a-zA-Z]{2,})\b',
            ]

            # æ–°å¢ï¼šå®Œæ•´åŸŸåæ ¼å¼ï¼ˆåŸŸåä¸è¢«DOTåˆ†éš”ï¼Œå¦‚ user [AT] mail.nankai.edu.cnï¼‰
            complete_domain_patterns = [
                # å¸¦æ‹¬å·çš„å®Œæ•´åŸŸåæ ¼å¼
                r'\b([a-zA-Z0-9._%+-]+)\s*\[AT\]\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s*\(AT\)\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s*\(at\)\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b',

                # åŸºæœ¬çš„å®Œæ•´åŸŸåæ ¼å¼
                r'\b([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b',
                r'\b([a-zA-Z0-9._%+-]+)\s+at\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b',
            ]

            print(f"ğŸ” åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢æ··æ·†é‚®ç®±...")

            # å¤„ç†ä¼ ç»Ÿçš„ä¸‰ç»„æ ¼å¼ï¼ˆuser AT domain DOT tldï¼‰
            for pattern in obfuscated_patterns:
                matches = re.finditer(pattern, page_text, re.IGNORECASE)

                for match in matches:
                    try:
                        if len(match.groups()) == 3:
                            # åŸºæœ¬æ ¼å¼ï¼šuser AT domain DOT tld
                            user_part = match.group(1).strip()
                            domain_part = match.group(2).strip()
                            tld_part = match.group(3).strip()

                            # å¤„ç†åŸŸåéƒ¨åˆ†å¯èƒ½åŒ…å«å¤šä¸ª dot çš„æƒ…å†µ
                            domain_part = domain_part.replace(' dot ', '.').replace(' DOT ', '.')

                            # æ„å»ºé‚®ç®±
                            email = f"{user_part}@{domain_part}.{tld_part}"
                            emails.append(email)

                            print(f"âœ… æ‰¾åˆ°æ··æ·†é‚®ç®±ï¼ˆä¸‰ç»„æ ¼å¼ï¼‰: {match.group(0)} â†’ {email}")

                    except Exception as e:
                        print(f"âš ï¸ è§£ææ··æ·†é‚®ç®±æ—¶å‡ºé”™: {e}")
                        continue

            # å¤„ç†å®Œæ•´åŸŸåæ ¼å¼ï¼ˆuser [AT] complete.domain.comï¼‰
            for pattern in complete_domain_patterns:
                matches = re.finditer(pattern, page_text, re.IGNORECASE)

                for match in matches:
                    try:
                        if len(match.groups()) == 2:
                            # å®Œæ•´åŸŸåæ ¼å¼ï¼šuser [AT] complete.domain.com
                            user_part = match.group(1).strip()
                            domain_part = match.group(2).strip()

                            # ç›´æ¥æ„å»ºé‚®ç®±
                            email = f"{user_part}@{domain_part}"
                            emails.append(email)

                            print(f"âœ… æ‰¾åˆ°æ··æ·†é‚®ç®±ï¼ˆå®Œæ•´åŸŸåæ ¼å¼ï¼‰: {match.group(0)} â†’ {email}")

                    except Exception as e:
                        print(f"âš ï¸ è§£æå®Œæ•´åŸŸåæ··æ·†é‚®ç®±æ—¶å‡ºé”™: {e}")
                        continue

            # ç‰¹æ®Šå¤„ç†ï¼šæŸ¥æ‰¾ "Email:" åé¢çš„æ··æ·†æ ¼å¼
            # ä¸‰ç»„æ ¼å¼ï¼ˆåŸŸåè¢«DOTåˆ†éš”ï¼‰
            email_patterns_3groups = [
                r'Email:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+(?:\s+dot\s+[a-zA-Z0-9.-]+)*)\s+dot\s+([a-zA-Z]{2,})',
                r'E-mail:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+(?:\s+dot\s+[a-zA-Z0-9.-]+)*)\s+dot\s+([a-zA-Z]{2,})',
                r'Contact:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+(?:\s+dot\s+[a-zA-Z0-9.-]+)*)\s+dot\s+([a-zA-Z]{2,})',
            ]

            for pattern in email_patterns_3groups:
                matches = re.finditer(pattern, page_text, re.IGNORECASE)

                for match in matches:
                    try:
                        user_part = match.group(1).strip()
                        domain_part = match.group(2).strip()
                        tld_part = match.group(3).strip()

                        # å¤„ç†åŸŸåéƒ¨åˆ†
                        domain_part = domain_part.replace(' dot ', '.').replace(' DOT ', '.')

                        # æ„å»ºé‚®ç®±
                        email = f"{user_part}@{domain_part}.{tld_part}"
                        emails.append(email)

                        print(f"âœ… ä»Emailæ ‡ç­¾æ‰¾åˆ°æ··æ·†é‚®ç®±ï¼ˆä¸‰ç»„æ ¼å¼ï¼‰: {match.group(0)} â†’ {email}")

                    except Exception as e:
                        print(f"âš ï¸ è§£æEmailæ ‡ç­¾æ··æ·†é‚®ç®±æ—¶å‡ºé”™: {e}")
                        continue

            # ä¸¤ç»„æ ¼å¼ï¼ˆå®Œæ•´åŸŸåï¼‰
            email_patterns_2groups = [
                r'Email:\s*([a-zA-Z0-9._%+-]+)\s*\[AT\]\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'E-mail:\s*([a-zA-Z0-9._%+-]+)\s*\[AT\]\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'Contact:\s*([a-zA-Z0-9._%+-]+)\s*\[AT\]\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'Email:\s*([a-zA-Z0-9._%+-]+)\s*\(AT\)\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'E-mail:\s*([a-zA-Z0-9._%+-]+)\s*\(AT\)\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'Contact:\s*([a-zA-Z0-9._%+-]+)\s*\(AT\)\s*([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'Email:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'E-mail:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
                r'Contact:\s*([a-zA-Z0-9._%+-]+)\s+AT\s+([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            ]

            for pattern in email_patterns_2groups:
                matches = re.finditer(pattern, page_text, re.IGNORECASE)

                for match in matches:
                    try:
                        user_part = match.group(1).strip()
                        domain_part = match.group(2).strip()

                        # ç›´æ¥æ„å»ºé‚®ç®±
                        email = f"{user_part}@{domain_part}"
                        emails.append(email)

                        print(f"âœ… ä»Emailæ ‡ç­¾æ‰¾åˆ°æ··æ·†é‚®ç®±ï¼ˆå®Œæ•´åŸŸåæ ¼å¼ï¼‰: {match.group(0)} â†’ {email}")

                    except Exception as e:
                        print(f"âš ï¸ è§£æEmailæ ‡ç­¾å®Œæ•´åŸŸåæ··æ·†é‚®ç®±æ—¶å‡ºé”™: {e}")
                        continue

            # å»é‡
            unique_emails = list(set(emails))

            if unique_emails:
                print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(unique_emails)} ä¸ªæ··æ·†æ ¼å¼é‚®ç®±")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ··æ·†æ ¼å¼é‚®ç®±")

            return unique_emails

        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾æ··æ·†é‚®ç®±æ—¶å‡ºé”™: {e}")
            return []

    def _find_merged_emails(self, soup: BeautifulSoup) -> List[str]:
        """
        æŸ¥æ‰¾åˆå¹¶æ ¼å¼çš„é‚®ç®±

        æ”¯æŒçš„æ ¼å¼ï¼š
        - {user1,user2,user3}@domain.com
        - {shawnxxh,chongyangtao,hishentao}@gmail.com
        - {minglii,tianyi}@umd.edu

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            å±•å¼€åçš„é‚®ç®±åˆ—è¡¨
        """
        emails = []

        try:
            # è·å–é¡µé¢æ–‡æœ¬
            page_text = soup.get_text()

            print(f"ğŸ” åœ¨é¡µé¢æ–‡æœ¬ä¸­æœç´¢åˆå¹¶æ ¼å¼é‚®ç®±...")

            # åˆå¹¶æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
            # åŒ¹é… {user1,user2,user3}@domain.com æ ¼å¼
            merged_pattern = r'\{([a-zA-Z0-9._-]+(?:,[a-zA-Z0-9._-]+)*)\}@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})'

            matches = re.finditer(merged_pattern, page_text, re.IGNORECASE)

            for match in matches:
                try:
                    users_part = match.group(1)  # user1,user2,user3
                    domain_part = match.group(2)  # domain.com

                    # åˆ†å‰²ç”¨æˆ·å
                    usernames = [username.strip() for username in users_part.split(',')]

                    print(f"ğŸ¯ æ‰¾åˆ°åˆå¹¶æ ¼å¼: {{{users_part}}}@{domain_part}")
                    print(f"ğŸ“§ åŒ…å« {len(usernames)} ä¸ªç”¨æˆ·å: {usernames}")

                    # ä¸ºæ¯ä¸ªç”¨æˆ·åç”Ÿæˆé‚®ç®±
                    for username in usernames:
                        if username:  # ç¡®ä¿ç”¨æˆ·åä¸ä¸ºç©º
                            email = f"{username}@{domain_part}"
                            emails.append(email)
                            print(f"   âœ… å±•å¼€é‚®ç®±: {email}")

                except Exception as e:
                    print(f"âš ï¸ å¤„ç†åˆå¹¶æ ¼å¼æ—¶å‡ºé”™: {e}")
                    continue

            # å»é‡
            unique_emails = list(set(emails))

            if unique_emails:
                print(f"ğŸ¯ æ€»å…±å±•å¼€ {len(unique_emails)} ä¸ªåˆå¹¶æ ¼å¼é‚®ç®±")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°åˆå¹¶æ ¼å¼é‚®ç®±")

            return unique_emails

        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾åˆå¹¶æ ¼å¼é‚®ç®±æ—¶å‡ºé”™: {e}")
            return []


# æµ‹è¯•å‡½æ•°
async def test_email_finder():
    """æµ‹è¯•é‚®ç®±æŸ¥æ‰¾å™¨"""
    print("ğŸ§ª æµ‹è¯•RealEmailFinder")
    print("=" * 50)
    
    test_url = "https://scholar.google.com/citations?user=zF9dr1sAAAAJ&hl=zh-CN&oi=sra"
    
    async with RealEmailFinder() as finder:
        # æµ‹è¯•æå–ä¸ªäººä¸»é¡µ
        homepage = await finder._get_personal_website_from_scholar_profile(test_url)
        print(f"ä¸ªäººä¸»é¡µ: {homepage}")
        
        if homepage:
            # æµ‹è¯•æå–é‚®ç®±
            emails = await finder._extract_emails_from_website(homepage)
            print(f"æ‰¾åˆ°çš„é‚®ç®±: {emails}")


if __name__ == "__main__":
    asyncio.run(test_email_finder())
