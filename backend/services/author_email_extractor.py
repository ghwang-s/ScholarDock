#!/usr/bin/env python3
"""
ä½œè€…é‚®ç®±æå–æœåŠ¡
åŸºäºç°æœ‰çš„RealEmailFinderï¼Œä¸“é—¨ç”¨äºæ‰¹é‡æå–è®ºæ–‡ä½œè€…é‚®ç®±
"""
import asyncio
import os
from typing import List, Dict, Optional
from .real_email_finder import RealEmailFinder
from .pdf_email_extractor import PDFEmailExtractor


class AuthorEmailExtractor:
    """ä½œè€…é‚®ç®±æå–å™¨"""

    def __init__(self, proxy: Optional[str] = None):
        """
        åˆå§‹åŒ–é‚®ç®±æå–å™¨

        Args:
            proxy: ä»£ç†è®¾ç½®ï¼Œä¾‹å¦‚ "http://127.0.0.1:7890"
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ä»£ç†ï¼Œç„¶åä½¿ç”¨é…ç½®ç®¡ç†å™¨
        if proxy:
            self.proxy = proxy
        else:
            # å¯¼å…¥ä»£ç†é…ç½®ç®¡ç†å™¨
            try:
                from backend.core.proxy_config import get_proxy
                self.proxy = get_proxy()
            except ImportError:
                # å›é€€åˆ°ç¯å¢ƒå˜é‡
                self.proxy = os.environ.get("SCHOLARDOCK_PROXY", "http://127.0.0.1:7890")

        self.email_finder = None
        self.pdf_extractor = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.email_finder = RealEmailFinder(proxy=self.proxy)
        await self.email_finder.__aenter__()

        self.pdf_extractor = PDFEmailExtractor(proxy=self.proxy)
        await self.pdf_extractor.__aenter__()

        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.email_finder:
            await self.email_finder.__aexit__(exc_type, exc_val, exc_tb)
        if self.pdf_extractor:
            await self.pdf_extractor.__aexit__(exc_type, exc_val, exc_tb)
    
    async def extract_author_emails(self, author_links: List[Dict[str, str]], progress_callback=None) -> Dict:
        """
        æ‰¹é‡æå–ä½œè€…é‚®ç®±

        Args:
            author_links: ä½œè€…é“¾æ¥åˆ—è¡¨ [{"name": "ä½œè€…å", "scholar_url": "Google Scholaré“¾æ¥"}]
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¸€ä¸ªåŒ…å«ä½œè€…é‚®ç®±å’ŒPDFå›é€€é‚®ç®±çš„å­—å…¸
        """
        if not self.email_finder:
            raise RuntimeError("EmailFinder not initialized. Use async context manager.")

        author_emails = []

        print(f"ğŸ” å¼€å§‹æå– {len(author_links)} ä¸ªä½œè€…çš„é‚®ç®±...")

        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹æå–
        if progress_callback:
            await progress_callback({
                "step": "start_extraction",
                "title": "å¼€å§‹é‚®ç®±æå–",
                "description": f"å¼€å§‹æå– {len(author_links)} ä¸ªä½œè€…çš„é‚®ç®±",
                "status": "in_progress"
            })

        # ç¬¬ä¸€é˜¶æ®µï¼šå°è¯•ä»ä¸ªäººä¸»é¡µæå–æ‰€æœ‰ä½œè€…çš„é‚®ç®±
        for author_info in author_links:
            author_name = author_info.get('name', '')
            scholar_url = author_info.get('scholar_url', '')

            if not scholar_url:
                print(f"âš ï¸ ä½œè€… {author_name} æ²¡æœ‰Google Scholaré“¾æ¥")
                author_emails.append({
                    'name': author_name,
                    'email': None,
                    'email_source': 'no_scholar_link'
                })
                continue

            try:
                print(f"ğŸ” æ­£åœ¨æå–ä½œè€… {author_name} çš„é‚®ç®±...")
                print(f"ğŸ”— Google Scholaré“¾æ¥: {scholar_url}")

                # æ›´æ–°è¿›åº¦ï¼šæå–ä¸ªäººä¸»é¡µ
                if progress_callback:
                    await progress_callback({
                        "step": "extract_personal_homepage",
                        "title": "æå–ä¸ªäººä¸»é¡µ",
                        "description": f"æ­£åœ¨æå–ä½œè€… {author_name} çš„ä¸ªäººä¸»é¡µé“¾æ¥",
                        "status": "in_progress"
                    })

                # ä»Google Scholarä¸ªäººä¸»é¡µæå–ä¸ªäººç½‘ç«™é“¾æ¥
                personal_homepage = await self.email_finder._get_personal_website_from_scholar_profile(scholar_url)

                if personal_homepage:
                    print(f"ğŸ  æ‰¾åˆ°ä¸ªäººä¸»é¡µ: {personal_homepage}")

                    # æ›´æ–°è¿›åº¦ï¼šä»ä¸ªäººç½‘ç«™æå–é‚®ç®±
                    if progress_callback:
                        await progress_callback({
                            "step": "extract_from_website",
                            "title": "ä»ä¸ªäººç½‘ç«™æå–é‚®ç®±",
                            "description": f"æ­£åœ¨ä» {author_name} çš„ä¸ªäººç½‘ç«™æå–é‚®ç®±",
                            "status": "in_progress"
                        })

                    # ä»ä¸ªäººç½‘ç«™æå–é‚®ç®±
                    emails = await self.email_finder._extract_emails_from_website(personal_homepage)

                    if emails:
                        primary_email = emails[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé‚®ç®±ä½œä¸ºä¸»è¦é‚®ç®±
                        print(f"âœ… æˆåŠŸæå–é‚®ç®±: {primary_email}")

                        author_emails.append({
                            'name': author_name,
                            'email': primary_email,
                            'email_source': 'personal_website',
                            'homepage': personal_homepage
                        })
                    else:
                        print(f"âš ï¸ ä½œè€… {author_name} çš„ä¸ªäººä¸»é¡µä¸­æœªæ‰¾åˆ°é‚®ç®±ï¼Œå°è¯•PDFå›é€€...")
                        # ä¸ªäººä¸»é¡µæœªæ‰¾åˆ°ï¼Œç«‹å³ä¸ºè¯¥ä½œè€…å°è¯•PDFå›é€€
                        pdf_emails = await self._extract_emails_from_pdf_fallback([author_info], progress_callback)
                        if pdf_emails:
                            primary_email = pdf_emails[0]
                            print(f"âœ… PDFå›é€€æˆåŠŸï¼Œæ‰¾åˆ°é‚®ç®±: {primary_email}")
                            author_emails.append({
                                'name': author_name,
                                'email': primary_email,
                                'email_source': 'pdf_fallback',
                                'homepage': personal_homepage
                            })
                        else:
                            print(f"âš ï¸ PDFå›é€€ä¹Ÿæœªæ‰¾åˆ°ä½œè€… {author_name} çš„é‚®ç®±")
                            author_emails.append({
                                'name': author_name,
                                'email': None,
                                'email_source': 'not_found_in_pdf',
                                'homepage': personal_homepage
                            })
                else:
                    print(f"âš ï¸ ä½œè€… {author_name} æ²¡æœ‰è®¾ç½®ä¸ªäººä¸»é¡µï¼Œå°è¯•PDFå›é€€...")
                    # æ²¡æœ‰ä¸ªäººä¸»é¡µï¼Œç«‹å³ä¸ºè¯¥ä½œè€…å°è¯•PDFå›é€€
                    pdf_emails = await self._extract_emails_from_pdf_fallback([author_info], progress_callback)
                    if pdf_emails:
                        primary_email = pdf_emails[0]
                        print(f"âœ… PDFå›é€€æˆåŠŸï¼Œæ‰¾åˆ°é‚®ç®±: {primary_email}")
                        author_emails.append({
                            'name': author_name,
                            'email': primary_email,
                            'email_source': 'pdf_fallback',
                            'homepage': None  # æ²¡æœ‰ä¸ªäººä¸»é¡µ
                        })
                    else:
                        print(f"âš ï¸ PDFå›é€€ä¹Ÿæœªæ‰¾åˆ°ä½œè€… {author_name} çš„é‚®ç®±")
                        author_emails.append({
                            'name': author_name,
                            'email': None,
                            'email_source': 'no_homepage_and_not_in_pdf',
                            'homepage': None
                        })

            except Exception as e:
                print(f"âŒ æå–ä½œè€… {author_name} é‚®ç®±æ—¶å‡ºé”™: {e}")
                author_emails.append({
                    'name': author_name,
                    'email': None,
                    'email_source': 'error'
                })

        print(f"\nğŸ“Š æœ€ç»ˆé‚®ç®±æå–ç»“æœ:")
        final_successful = sum(1 for email_info in author_emails if email_info.get('email'))
        print(f"âœ… æˆåŠŸæå–: {final_successful}/{len(author_links)}")
        
        final_result = {
            "author_emails": author_emails
        }

        # æ›´æ–°è¿›åº¦ï¼šå®Œæˆæå–
        if progress_callback:
            await progress_callback({
                "step": "extraction_complete",
                "title": "é‚®ç®±æå–å®Œæˆ",
                "description": f"å®Œæˆ {len(author_links)} ä¸ªä½œè€…çš„é‚®ç®±æå–ï¼ŒæˆåŠŸ {final_successful} ä¸ª",
                "status": "completed",
                "result": {
                    "successful_extractions": final_successful,
                    "total_authors": len(author_links),
                    "author_emails": author_emails,
                }
            })

        for email_info in author_emails:
            name = email_info.get('name', 'æœªçŸ¥')
            email = email_info.get('email', 'None')
            source = email_info.get('email_source', 'æœªçŸ¥')
            print(f"ğŸ‘¤ {name}: {email} ({source})")
        
        # è¿”å›åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        return final_result

    async def _extract_emails_from_pdf_fallback(self, author_links: List[Dict[str, str]], progress_callback=None) -> List[str]:
        """
        PDFå›é€€åŠŸèƒ½ï¼šä»æ‰€æœ‰ä½œè€…çš„è®ºæ–‡PDFä¸­æå–æ‰€æœ‰ä¸é‡å¤çš„é‚®ç®±ã€‚

        Args:
            author_links: ä½œè€…é“¾æ¥åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            ä¸€ä¸ªåŒ…å«æ‰€æœ‰ä»PDFä¸­æ‰¾åˆ°çš„ã€å»é‡åçš„é‚®ç®±åˆ—è¡¨ã€‚
        """
        try:
            print(f"ğŸ” å°è¯•ä¸º {len(author_links)} ä¸ªä½œè€…ä»PDFä¸­æå–é‚®ç®±...")
            
            all_pdf_emails = set()

            # ä¸ºæ¯ä¸ªä½œè€…å°è¯•PDFå›é€€
            for author_info in author_links:
                author_name = author_info.get('name')
                scholar_url = author_info.get('scholar_url')

                if not scholar_url or not author_name:
                    continue

                # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹å•ä¸ªä½œè€…çš„PDFæå–
                if progress_callback:
                    await progress_callback({
                        "step": "pdf_fallback_author",
                        "title": f"PDFå›é€€: {author_name}",
                        "description": f"æ­£åœ¨ä¸ºä½œè€… {author_name} æŸ¥æ‰¾è®ºæ–‡PDF",
                        "status": "in_progress"
                    })

                # è·å–è¯¥ä½œè€…çš„PDFé“¾æ¥
                pdf_urls = await self._get_author_pdf_urls(scholar_url)
                if not pdf_urls:
                    print(f"âš ï¸ æœªæ‰¾åˆ°ä½œè€… {author_name} çš„PDFé“¾æ¥")
                    continue

                # å°è¯•ä»è¯¥ä½œè€…çš„æ‰€æœ‰PDFä¸­æå–é‚®ç®±
                for i, pdf_url in enumerate(pdf_urls[:3]):  # æœ€å¤šå°è¯•3ä¸ªPDF
                    try:
                        print(f"\n{'='*20} PROCESSING PDF URL {'='*20}")
                        print(f"URL: {pdf_url}")
                        print(f"{'='*58}\n")
                        emails = await self.pdf_extractor.extract_emails_from_pdf_url(pdf_url)
                        if emails:
                            print(f"âœ… ä»PDF {pdf_url} æ‰¾åˆ° {len(emails)} ä¸ªé‚®ç®±")
                            for email in emails:
                                if len(all_pdf_emails) < 3:
                                    all_pdf_emails.add(email)
                                else:
                                    break
                            if len(all_pdf_emails) >= 3:
                                break
                    except Exception as e:
                        print(f"âŒ å¤„ç†PDF {pdf_url} å¤±è´¥: {e}")
                        continue
                
                if len(all_pdf_emails) >= 3:
                    break

            # æ›´æ–°è¿›åº¦ï¼šå®Œæˆ
            final_emails = list(all_pdf_emails)
            if progress_callback:
                await progress_callback({
                    "step": "pdf_fallback_complete",
                    "title": "PDFå›é€€å®Œæˆ",
                    "description": f"ä»PDFä¸­æ€»å…±æ‰¾åˆ° {len(final_emails)} ä¸ªç‹¬ç«‹é‚®ç®±",
                    "status": "completed"
                })
            
            print(f"âœ… PDFå›é€€å®Œæˆï¼Œè¿”å› {len(final_emails)} ä¸ªé‚®ç®±ã€‚")
            return final_emails

        except Exception as e:
            print(f"âŒ PDFå›é€€åŠŸèƒ½å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ“ PDFå›é€€é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return []

    async def _get_author_pdf_urls(self, scholar_url: str) -> List[str]:
        """
        ä»Google Scholarä½œè€…é¡µé¢è·å–è®ºæ–‡PDFé“¾æ¥

        Args:
            scholar_url: Google Scholarä½œè€…é“¾æ¥

        Returns:
            PDFé“¾æ¥åˆ—è¡¨
        """
        try:
            print(f"ğŸ” ä»Google Scholarè·å–PDFé“¾æ¥...")

            # è®¿é—®ä½œè€…çš„Google Scholaré¡µé¢
            async with self.email_finder.session.get(scholar_url, proxy=self.proxy) as response:
                if response.status != 200:
                    print(f"âŒ æ— æ³•è®¿é—®Google Scholaré¡µé¢ï¼ŒçŠ¶æ€ç : {response.status}")
                    return []

                html_content = await response.text()

                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')

                pdf_urls = []

                # æ–¹æ³•1: æŸ¥æ‰¾metaæ ‡ç­¾ä¸­çš„PDFé“¾æ¥
                print(f"ğŸ“ æ–¹æ³•1: æŸ¥æ‰¾metaæ ‡ç­¾ä¸­çš„citation_pdf_url...")
                meta_tags = soup.find_all('meta', attrs={'name': 'citation_pdf_url'})
                for meta in meta_tags:
                    pdf_url = meta.get('content')
                    if pdf_url:
                        # ç¡®ä¿æ˜¯å®Œæ•´çš„URL
                        if not pdf_url.startswith('http'):
                            pdf_url = f"http:{pdf_url}" if pdf_url.startswith('//') else f"http://{pdf_url}"
                        pdf_urls.append(pdf_url)
                        print(f"âœ… ä»metaæ ‡ç­¾æ‰¾åˆ°PDF: {pdf_url}")

                # æ–¹æ³•2: æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜é“¾æ¥ä¸­çš„PDF
                print(f"ğŸ“ æ–¹æ³•2: æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜é“¾æ¥...")
                title_links = soup.find_all('a', class_='gsc_a_at')  # Google Scholarè®ºæ–‡æ ‡é¢˜é“¾æ¥
                for link in title_links[:5]:  # åªæ£€æŸ¥å‰5ç¯‡è®ºæ–‡
                    href = link.get('href')
                    if href:
                        try:
                            # è®¿é—®è®ºæ–‡è¯¦æƒ…é¡µé¢æŸ¥æ‰¾PDFé“¾æ¥
                            paper_url = href if href.startswith('http') else f"https://scholar.google.com{href}"
                            print(f"ğŸ” æ£€æŸ¥è®ºæ–‡é¡µé¢: {paper_url}")

                            async with self.email_finder.session.get(paper_url, proxy=self.proxy) as paper_response:
                                if paper_response.status == 200:
                                    paper_html = await paper_response.text()
                                    paper_soup = BeautifulSoup(paper_html, 'html.parser')

                                    # åœ¨è®ºæ–‡é¡µé¢æŸ¥æ‰¾PDFé“¾æ¥
                                    pdf_links = paper_soup.find_all('a', href=True)
                                    for pdf_link in pdf_links:
                                        pdf_href = pdf_link.get('href')
                                        if pdf_href and ('.pdf' in pdf_href.lower() or 'arxiv.org/pdf' in pdf_href):
                                            if not pdf_href.startswith('http'):
                                                pdf_href = f"https:{pdf_href}" if pdf_href.startswith('//') else f"https://{pdf_href}"
                                            pdf_urls.append(pdf_href)
                                            print(f"âœ… ä»è®ºæ–‡é¡µé¢æ‰¾åˆ°PDF: {pdf_href}")
                                            break  # æ¯ç¯‡è®ºæ–‡åªå–ç¬¬ä¸€ä¸ªPDFé“¾æ¥
                        except Exception as e:
                            print(f"âš ï¸ æ£€æŸ¥è®ºæ–‡é¡µé¢å¤±è´¥: {e}")
                            continue

                # æ–¹æ³•3: ç›´æ¥åœ¨å½“å‰é¡µé¢æŸ¥æ‰¾PDFé“¾æ¥
                print(f"ğŸ“ æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾PDFé“¾æ¥...")
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href')
                    if href and ('.pdf' in href.lower() or 'arxiv.org/pdf' in href):
                        # å¤„ç†ç›¸å¯¹é“¾æ¥
                        if href.startswith('/'):
                            href = f"https://scholar.google.com{href}"
                        elif href.startswith('//'):
                            href = f"https:{href}"
                        elif not href.startswith('http'):
                            href = f"https://{href}"

                        pdf_urls.append(href)
                        print(f"âœ… ç›´æ¥æ‰¾åˆ°PDFé“¾æ¥: {href}")

                # å»é‡å¹¶é™åˆ¶æ•°é‡
                unique_pdf_urls = list(dict.fromkeys(pdf_urls))  # ä¿æŒé¡ºåºçš„å»é‡
                limited_pdf_urls = unique_pdf_urls[:5]  # æœ€å¤š5ä¸ªPDF

                print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(limited_pdf_urls)} ä¸ªå”¯ä¸€PDFé“¾æ¥")
                for i, url in enumerate(limited_pdf_urls):
                    print(f"  {i+1}. {url}")

                return limited_pdf_urls

        except Exception as e:
            print(f"âŒ è·å–PDFé“¾æ¥å¤±è´¥: {e}")
            return []
    

    
    async def extract_single_author_email(self, author_name: str, scholar_url: str) -> Optional[Dict[str, str]]:
        """
        æå–å•ä¸ªä½œè€…çš„é‚®ç®±
        
        Args:
            author_name: ä½œè€…åå­—
            scholar_url: Google Scholaré“¾æ¥
        
        Returns:
            ä½œè€…é‚®ç®±ä¿¡æ¯æˆ–None
        """
        author_links = [{'name': author_name, 'scholar_url': scholar_url}]
        emails = await self.extract_author_emails(author_links)
        return emails[0] if emails else None


# ä¾¿æ·å‡½æ•°
async def extract_authors_emails_from_links(author_links: List[Dict[str, str]], 
                                          proxy: Optional[str] = None) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»ä½œè€…é“¾æ¥åˆ—è¡¨æå–é‚®ç®±
    
    Args:
        author_links: ä½œè€…é“¾æ¥åˆ—è¡¨
        proxy: ä»£ç†è®¾ç½®
    
    Returns:
        ä½œè€…é‚®ç®±åˆ—è¡¨
    """
    async with AuthorEmailExtractor(proxy=proxy) as extractor:
        return await extractor.extract_author_emails(author_links)


# æµ‹è¯•å‡½æ•°
async def test_author_email_extraction():
    """æµ‹è¯•ä½œè€…é‚®ç®±æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä½œè€…é‚®ç®±æå–åŠŸèƒ½")
    print("=" * 60)

    # æµ‹è¯•æ•°æ®
    test_author_links = [
        {
            'name': 'æµ‹è¯•ä½œè€…1 (æœ‰ä¸»é¡µ)',
            'scholar_url': 'https://scholar.google.com/citations?user=zF9dr1sAAAAJ&hl=zh-CN&oi=sra'
        },
        {
            'name': 'Alex Krizhevsky (æ— ä¸»é¡µ)',
            'scholar_url': 'https://scholar.google.com/citations?user=JicYpd0AAAAJ&hl=en'
        }
    ]

    try:
        async with AuthorEmailExtractor() as extractor:
            emails = await extractor.extract_author_emails(test_author_links)

            print(f"\nğŸ“Š æå–ç»“æœ:")
            author_emails_list = emails.get('author_emails', [])
            if author_emails_list:
                for email_info in author_emails_list:
                    print(f"ğŸ‘¤ ä½œè€…: {email_info.get('name', 'N/A')}")
                    print(f"ğŸ“§ é‚®ç®±: {email_info.get('email', 'N/A')}")
                    print(f"ğŸ”— æ¥æº: {email_info.get('email_source', 'N/A')}")
                    if 'homepage' in email_info and email_info['homepage']:
                        print(f"ğŸ  ä¸»é¡µ: {email_info['homepage']}")
                    print("-" * 40)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çœŸå®é‚®ç®±")

            return emails

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_author_email_extraction())
