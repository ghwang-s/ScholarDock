from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.orm import selectinload
from typing import List, Optional, Dict, Any
import os
import asyncio
from pydantic import BaseModel, EmailStr

from core.config import settings
from core.database import init_db, get_db
from core.proxy_config import get_proxy
from models.article import SearchRequest, SearchResponse, SearchDB, ArticleDB, SearchSchema, ArticleSchema
from services.original_spider import OriginalScholarSpider
from core.proxy_config import get_proxy
import requests
from services.export import ExportService
from services.author_email_extractor import AuthorEmailExtractor
from services.email_sender import get_email_sender
from websocket_server import setup_websocket_endpoint, send_progress_update


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown


app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    debug=settings.debug,
    lifespan=lifespan
)

# è®¾ç½®WebSocketç«¯ç‚¹
setup_websocket_endpoint(app)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "version": settings.app_version}


@app.post("/api/search", response_model=SearchResponse)
async def search_articles(
    request: SearchRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    search_record = SearchDB(
        keyword=request.keyword,
        start_year=request.start_year,
        end_year=request.end_year
    )
    db.add(search_record)
    await db.commit()
    await db.refresh(search_record)
    
    try:
        print(f"ğŸ” å¼€å§‹æœç´¢: '{request.keyword}', ç›®æ ‡ç»“æœæ•°: {request.num_results}")

        # å¦‚æœå¯ç”¨äº†æ’é‡ï¼Œåˆ™ä»æ•°æ®åº“è·å–æ‰€æœ‰ç°æœ‰æ ‡é¢˜
        existing_titles = set()
        if request.exclude_duplicates:
            result = await db.execute(select(ArticleDB.title))
            all_titles = result.scalars().all()
            existing_titles = {title.lower().strip() for title in all_titles if title}
            print(f"ğŸ“š ä»æ•°æ®åº“åŠ è½½äº† {len(existing_titles)} ä¸ªå·²å­˜åœ¨çš„æ ‡é¢˜ç”¨äºå»é‡")

        async with OriginalScholarSpider() as spider:
            articles = await spider.search(
                keyword=request.keyword,
                num_results=request.num_results,
                start_year=request.start_year,
                end_year=request.end_year,
                filter_by_title=request.filter_by_title,
                exclude_duplicates=request.exclude_duplicates,
                existing_titles=existing_titles
            )

        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        
        # Return empty results if nothing found
        if not articles:
            print(f"No results found for '{request.keyword}' - may be blocked by Google Scholar")
        
        if request.sort_by == "citations":
            articles.sort(key=lambda x: x.citations, reverse=True)
        elif request.sort_by == "citations_per_year":
            articles.sort(key=lambda x: x.citations_per_year, reverse=True)
        elif request.sort_by == "year":
            articles.sort(key=lambda x: x.year or 0, reverse=True)
        
        for article in articles:
            article_db = ArticleDB(
                title=article.title,
                authors=article.authors,
                author_links=article.author_links,  # ä¿å­˜ä½œè€…é“¾æ¥ä¿¡æ¯
                venue=article.venue,
                publisher=article.publisher,
                year=article.year,
                citations=article.citations,
                citations_per_year=article.citations_per_year,
                description=article.description,
                url=article.url,
                search_id=search_record.id
            )
            db.add(article_db)
        
        search_record.total_results = len(articles)
        await db.commit()
        
        return SearchResponse(
            search_id=search_record.id,
            keyword=request.keyword,
            total_results=len(articles),
            articles=articles
        )
        
    except Exception as e:
        print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

        await db.rollback()

        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_message = str(e)
        if "timeout" in error_message.lower():
            error_message = "ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä»£ç†è®¾ç½®"
        elif "proxy" in error_message.lower():
            error_message = "ä»£ç†è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç†æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"
        elif "robot" in error_message.lower() or "unusual traffic" in error_message.lower():
            error_message = "è¢«Google Scholaræ£€æµ‹ä¸ºæœºå™¨äººï¼Œè¯·ç¨åå†è¯•æˆ–æ›´æ¢ä»£ç†"
        elif "é¢‘ç‡è¿‡é«˜" in error_message or "429" in error_message:
            error_message = "è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼ŒGoogle Scholaræš‚æ—¶é™åˆ¶äº†è®¿é—®ã€‚è¯·ç­‰å¾…5-10åˆ†é’Ÿåé‡è¯•ï¼Œæˆ–å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚"
        elif "connection" in error_message.lower():
            error_message = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
        else:
            error_message = f"æœç´¢å¤±è´¥: {error_message}"

        raise HTTPException(status_code=500, detail=error_message)


@app.get("/api/proxy/status")
async def get_proxy_status():
    """è·å–ä»£ç†çŠ¶æ€"""
    try:
        proxy = get_proxy()
        if not proxy:
            return {
                "status": "disabled",
                "message": "æœªé…ç½®ä»£ç†",
                "proxy": None
            }

        # æµ‹è¯•ä»£ç†è¿æ¥
        proxies = {
            'http': proxy,
            'https': proxy
        }

        test_url = "https://www.google.com"
        response = requests.get(test_url, proxies=proxies, timeout=10)

        if response.status_code == 200:
            return {
                "status": "connected",
                "message": "ä»£ç†è¿æ¥æ­£å¸¸",
                "proxy": proxy
            }
        else:
            return {
                "status": "error",
                "message": f"ä»£ç†è¿æ¥å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}",
                "proxy": proxy
            }

    except Exception as e:
        return {
            "status": "error",
            "message": f"ä»£ç†è¿æ¥å¤±è´¥: {str(e)}",
            "proxy": get_proxy()
        }


@app.get("/api/searches", response_model=List[SearchSchema])
async def get_search_history(
    skip: int = 0,
    limit: int = 20,
    db: AsyncSession = Depends(get_db)
):
    result = await db.execute(
        select(SearchDB)
        .options(selectinload(SearchDB.articles))
        .order_by(SearchDB.created_at.desc())
        .offset(skip)
        .limit(limit)
    )
    searches = result.scalars().all()
    return searches


@app.get("/api/search/{search_id}", response_model=SearchSchema)
async def get_search_details(
    search_id: int,
    db: AsyncSession = Depends(get_db)
):
    result = await db.execute(
        select(SearchDB)
        .options(selectinload(SearchDB.articles))
        .where(SearchDB.id == search_id)
    )
    search = result.scalar_one_or_none()
    
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    return search


@app.get("/api/export/{search_id}")
async def export_search_results(
    search_id: int,
    format: str = "csv",
    db: AsyncSession = Depends(get_db)
):
    result = await db.execute(
        select(SearchDB)
        .options(selectinload(SearchDB.articles))
        .where(SearchDB.id == search_id)
    )
    search = result.scalar_one_or_none()
    
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    articles = [ArticleSchema.model_validate(article) for article in search.articles]
    
    if format == "csv":
        content = ExportService.to_csv(articles)
        media_type = "text/csv"
        filename = f"scholar_results_{search.keyword}.csv"
    elif format == "json":
        content = ExportService.to_json(articles)
        media_type = "application/json"
        filename = f"scholar_results_{search.keyword}.json"
    elif format == "excel":
        content = ExportService.to_excel(articles)
        media_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        filename = f"scholar_results_{search.keyword}.xlsx"
    elif format == "bibtex":
        content = ExportService.to_bibtex(articles)
        media_type = "text/plain"
        filename = f"scholar_results_{search.keyword}.bib"
    else:
        raise HTTPException(status_code=400, detail="Invalid export format")
    
    return Response(
        content=content,
        media_type=media_type,
        headers={
            "Content-Disposition": f"attachment; filename={filename}"
        }
    )


@app.delete("/api/search/{search_id}")
async def delete_search(
    search_id: int,
    db: AsyncSession = Depends(get_db)
):
    result = await db.execute(
        select(SearchDB)
        .options(selectinload(SearchDB.articles))
        .where(SearchDB.id == search_id)
    )
    search = result.scalar_one_or_none()
    
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    await db.delete(search)
    await db.commit()
    
    return {"message": "Search deleted successfully"}


@app.post("/api/extract-author-emails/{article_id}")
async def extract_author_emails(
    article_id: int,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    """æå–æŒ‡å®šè®ºæ–‡çš„ä½œè€…é‚®ç®±"""
    # è·å–è®ºæ–‡ä¿¡æ¯
    # è·å–è®ºæ–‡ä¿¡æ¯
    result = await db.execute(
        select(ArticleDB).where(ArticleDB.id == article_id)
    )
    article = result.scalar_one_or_none()

    if not article:
        raise HTTPException(status_code=404, detail="Article not found")

    if not article.author_links:
        raise HTTPException(status_code=400, detail="No author links available for this article")

    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
    async def progress_callback(progress_data: dict):
        """è¿›åº¦å›è°ƒå‡½æ•°ï¼Œé€šè¿‡WebSocketå‘é€è¿›åº¦æ›´æ–°"""
        try:
            await send_progress_update(str(article_id), progress_data)
        except Exception as e:
            print(f"å‘é€è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

    try:
        # æå–ä½œè€…é‚®ç®±ï¼ˆä½¿ç”¨ä»£ç†ï¼‰
        proxy = get_proxy()
        print(f"ğŸ”§ ä½¿ç”¨ä»£ç†: {proxy}")
        async with AuthorEmailExtractor(proxy=proxy) as extractor:
            extraction_result = await extractor.extract_author_emails(article.author_links, progress_callback)

        # å¤„ç†è¿”å›ç»“æœ
        if isinstance(extraction_result, dict):
            author_emails = extraction_result.get('author_emails', [])
            pdf_fallback_emails = extraction_result.get('pdf_fallback_emails', [])
        else:
            # å…¼å®¹æ—§æ ¼å¼
            author_emails = extraction_result
            pdf_fallback_emails = []

        # æ›´æ–°æ•°æ®åº“
        article.author_emails = author_emails
        # å¦‚æœæœ‰PDFé‚®ç®±ï¼Œä¹Ÿä¿å­˜åˆ°æ•°æ®åº“
        if pdf_fallback_emails:
            article.pdf_fallback_emails = pdf_fallback_emails
        await db.commit()
        await db.refresh(article)

        # å‘é€å®Œæˆæ¶ˆæ¯
        await send_progress_update(str(article_id), {
            "type": "completion",
            "step": "extraction_complete",
            "title": "é‚®ç®±æå–å®Œæˆ",
            "description": f"æˆåŠŸæå– {len([e for e in author_emails if e.get('email')])} ä¸ªä½œè€…é‚®ç®±",
            "status": "completed",
            "result": {
                "author_emails": author_emails
            }
        })

        return {
            "message": "Author emails extracted successfully",
            "article_id": article_id,
            "author_emails": author_emails,
            "pdf_fallback_emails": pdf_fallback_emails
        }

    except Exception as e:
        await db.rollback()
        
        # å‘é€é”™è¯¯æ¶ˆæ¯
        await send_progress_update(str(article_id), {
            "type": "error",
            "step": "extraction_error",
            "title": "é‚®ç®±æå–å¤±è´¥",
            "description": f"æå–ä½œè€…é‚®ç®±æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "status": "failed"
        })
        
        raise HTTPException(status_code=500, detail=f"Failed to extract author emails: {str(e)}")


@app.post("/api/extract-all-author-emails/{search_id}")
async def extract_all_author_emails(
    search_id: int,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    """æå–æŒ‡å®šæœç´¢ç»“æœä¸­æ‰€æœ‰è®ºæ–‡çš„ä½œè€…é‚®ç®±"""
    # è·å–æœç´¢è®°å½•å’Œç›¸å…³è®ºæ–‡
    result = await db.execute(
        select(SearchDB)
        .options(selectinload(SearchDB.articles))
        .where(SearchDB.id == search_id)
    )
    search = result.scalar_one_or_none()

    if not search:
        raise HTTPException(status_code=404, detail="Search not found")

    if not search.articles:
        raise HTTPException(status_code=400, detail="No articles found for this search")

    # åœ¨åå°ä»»åŠ¡ä¸­å¤„ç†é‚®ç®±æå–
    background_tasks.add_task(
        _extract_emails_for_search,
        search_id,
        db
    )

    return {
        "message": "Author email extraction started in background",
        "search_id": search_id,
        "total_articles": len(search.articles)
    }


async def _extract_emails_for_search(search_id: int, db: AsyncSession):
    """åå°ä»»åŠ¡ï¼šä¸ºæœç´¢ç»“æœä¸­çš„æ‰€æœ‰è®ºæ–‡æå–ä½œè€…é‚®ç®±"""
    try:
        # é‡æ–°è·å–æœç´¢è®°å½•å’Œè®ºæ–‡
        result = await db.execute(
            select(SearchDB)
            .options(selectinload(SearchDB.articles))
            .where(SearchDB.id == search_id)
        )
        search = result.scalar_one_or_none()

        if not search or not search.articles:
            return

        proxy = get_proxy()
        print(f"ğŸ”§ æ‰¹é‡æå–ä½¿ç”¨ä»£ç†: {proxy}")
        async with AuthorEmailExtractor(proxy=proxy) as extractor:
            for article in search.articles:
                if article.author_links and not article.author_emails:
                    try:
                        print(f"ğŸ” æ­£åœ¨æå–è®ºæ–‡ '{article.title}' çš„ä½œè€…é‚®ç®±...")
                        extraction_result = await extractor.extract_author_emails(article.author_links)

                        # å¤„ç†è¿”å›ç»“æœ
                        if isinstance(extraction_result, dict):
                            author_emails = extraction_result.get('author_emails', [])
                            pdf_fallback_emails = extraction_result.get('pdf_fallback_emails', [])
                        else:
                            # å…¼å®¹æ—§æ ¼å¼
                            author_emails = extraction_result
                            pdf_fallback_emails = []

                        # æ›´æ–°è®ºæ–‡çš„ä½œè€…é‚®ç®±ä¿¡æ¯
                        article.author_emails = author_emails
                        # å¦‚æœæœ‰PDFé‚®ç®±ï¼Œä¹Ÿä¿å­˜åˆ°æ•°æ®åº“
                        if pdf_fallback_emails:
                            article.pdf_fallback_emails = pdf_fallback_emails
                        await db.commit()

                        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                        print("â³ ç­‰å¾…5ç§’åå¤„ç†ä¸‹ä¸€ç¯‡è®ºæ–‡...")
                        await asyncio.sleep(5)

                        print(f"âœ… æˆåŠŸæå– {len(author_emails)} ä¸ªä½œè€…é‚®ç®±")

                    except Exception as e:
                        print(f"âŒ æå–è®ºæ–‡ '{article.title}' ä½œè€…é‚®ç®±å¤±è´¥: {e}")
                        await db.rollback()
                        continue

        print(f"ğŸ‰ æœç´¢ {search_id} çš„ä½œè€…é‚®ç®±æå–å®Œæˆ")

    except Exception as e:
        print(f"âŒ åå°é‚®ç®±æå–ä»»åŠ¡å¤±è´¥: {e}")


@app.get("/api/proxy-status")
async def get_proxy_status():
    """è·å–ä»£ç†çŠ¶æ€"""
    proxy = get_proxy()

    if not proxy:
        return {
            "proxy_enabled": False,
            "proxy_url": None,
            "status": "disabled"
        }

    # æµ‹è¯•ä»£ç†è¿æ¥
    try:
        import aiohttp

        connector = aiohttp.TCPConnector()
        timeout = aiohttp.ClientTimeout(total=10)

        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        ) as session:
            async with session.get("https://www.google.com", proxy=proxy) as response:
                if response.status == 200:
                    return {
                        "proxy_enabled": True,
                        "proxy_url": proxy,
                        "status": "connected",
                        "message": "ä»£ç†è¿æ¥æ­£å¸¸"
                    }
                else:
                    return {
                        "proxy_enabled": True,
                        "proxy_url": proxy,
                        "status": "error",
                        "message": f"ä»£ç†è¿æ¥å¼‚å¸¸ (çŠ¶æ€ç : {response.status})"
                    }

    except Exception as e:
        return {
            "proxy_enabled": True,
            "proxy_url": proxy,
            "status": "error",
            "message": f"ä»£ç†è¿æ¥å¤±è´¥: {str(e)}"
        }


# é‚®ä»¶ç›¸å…³çš„Pydanticæ¨¡å‹
class EmailPreviewRequest(BaseModel):
    """é‚®ä»¶é¢„è§ˆè¯·æ±‚"""
    author_name: str
    paper_title: str
    paper_venue: Optional[str] = None
    paper_year: Optional[int] = None
    paper_citations: Optional[int] = None


class EmailSendRequest(BaseModel):
    """é‚®ä»¶å‘é€è¯·æ±‚"""
    to_email: EmailStr
    subject: str
    author_name: str
    paper_title: str
    paper_venue: Optional[str] = None
    paper_year: Optional[int] = None
    paper_citations: Optional[int] = None


class BatchEmailRequest(BaseModel):
    """æ‰¹é‡é‚®ä»¶å‘é€è¯·æ±‚"""
    search_id: int
    subject: str
    include_author_emails: bool = True
    include_pdf_emails: bool = True


@app.post("/api/email/preview")
async def preview_email(request: EmailPreviewRequest):
    """é¢„è§ˆé‚®ä»¶å†…å®¹"""
    try:
        sender = get_email_sender()

        template_data = {
            'author_name': request.author_name,
            'paper_title': request.paper_title,
            'paper_venue': request.paper_venue,
            'paper_year': request.paper_year,
            'paper_citations': request.paper_citations
        }

        html_content = sender.preview_email(template_data)

        return {
            'success': True,
            'html_content': html_content,
            'template_data': template_data
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‚®ä»¶é¢„è§ˆå¤±è´¥: {str(e)}")


@app.post("/api/email/send")
async def send_email(request: EmailSendRequest):
    """å‘é€é‚®ä»¶"""
    try:
        sender = get_email_sender()

        template_data = {
            'author_name': request.author_name,
            'paper_title': request.paper_title,
            'paper_venue': request.paper_venue,
            'paper_year': request.paper_year,
            'paper_citations': request.paper_citations
        }

        result = sender.send_email(
            to_email=request.to_email,
            subject=request.subject,
            template_data=template_data
        )

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")


@app.get("/api/email/config")
async def get_email_config():
    """è·å–é‚®ä»¶é…ç½®çŠ¶æ€"""
    try:
        sender = get_email_sender()
        return sender.validate_email_config()

    except Exception as e:
        return {
            'valid': False,
            'message': f'é‚®ä»¶é…ç½®é”™è¯¯: {str(e)}',
            'error': str(e)
        }


@app.post("/api/email/batch-send")
async def batch_send_emails(
    request: BatchEmailRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    """æ‰¹é‡å‘é€é‚®ä»¶"""
    try:
        # è·å–æœç´¢è®°å½•å’Œæ–‡ç« 
        result = await db.execute(
            select(SearchDB)
            .options(selectinload(SearchDB.articles))
            .where(SearchDB.id == request.search_id)
        )
        search = result.scalar_one_or_none()

        if not search:
            raise HTTPException(status_code=404, detail="Search not found")

        if not search.articles:
            raise HTTPException(status_code=400, detail="No articles found for this search")

        # è·å–ä¹‹å‰å·²ç»å‘é€è¿‡é‚®ä»¶çš„è®ºæ–‡æ ‡é¢˜
        previously_sent_titles = await _get_previously_sent_articles(request.search_id, db)

        # ç»Ÿè®¡é‚®ç®±æ•°é‡ï¼ˆæ’é™¤é‡å¤è®ºæ–‡ï¼‰
        total_emails = 0
        skipped_count = 0
        for article in search.articles:
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²ç»å‘é€è¿‡é‚®ä»¶
            normalized_title = article.title.lower().strip() if article.title else ""
            if normalized_title in previously_sent_titles:
                skipped_count += 1
                continue

            if request.include_author_emails and article.author_emails:
                total_emails += len([e for e in article.author_emails if e.get('email')])
            if request.include_pdf_emails and article.pdf_fallback_emails:
                total_emails += len(article.pdf_fallback_emails)

        print(f"ğŸ“Š æ‰¹é‡å‘é€ç»Ÿè®¡: æ€»è®ºæ–‡ {len(search.articles)} ç¯‡ï¼Œè·³è¿‡é‡å¤ {skipped_count} ç¯‡ï¼Œå¾…å‘é€é‚®ç®± {total_emails} ä¸ª")

        if total_emails == 0:
            raise HTTPException(status_code=400, detail="No emails found to send")

        # åœ¨åå°ä»»åŠ¡ä¸­å¤„ç†æ‰¹é‡å‘é€
        background_tasks.add_task(
            _batch_send_emails_task,
            request.search_id,
            request.subject,
            request.include_author_emails,
            request.include_pdf_emails,
            db
        )

        return {
            "message": "Batch email sending started",
            "search_id": request.search_id,
            "total_emails": total_emails,
            "total_articles": len(search.articles)
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start batch email sending: {str(e)}")


async def _get_previously_sent_articles(search_id: int, db: AsyncSession) -> set:
    """è·å–ä¹‹å‰æœç´¢ç»“æœä¸­å·²ç»å‘é€è¿‡é‚®ä»¶çš„è®ºæ–‡æ ‡é¢˜"""
    try:
        # è·å–å½“å‰æœç´¢ä¹‹å‰çš„æ‰€æœ‰æœç´¢è®°å½•
        result = await db.execute(
            select(SearchDB)
            .options(selectinload(SearchDB.articles))
            .where(SearchDB.id < search_id)
            .order_by(SearchDB.id.desc())
        )
        previous_searches = result.scalars().all()

        sent_article_titles = set()

        for search in previous_searches:
            for article in search.articles:
                # æ£€æŸ¥è¿™ç¯‡è®ºæ–‡æ˜¯å¦æœ‰é‚®ç®±ä¿¡æ¯ï¼ˆè¯´æ˜å¯èƒ½å·²ç»å‘é€è¿‡é‚®ä»¶ï¼‰
                if (article.author_emails and any(e.get('email') for e in article.author_emails)) or \
                   (article.pdf_fallback_emails and len(article.pdf_fallback_emails) > 0):
                    # ä½¿ç”¨æ ‡é¢˜çš„æ ‡å‡†åŒ–ç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒï¼ˆå»é™¤ç©ºæ ¼ã€è½¬å°å†™ï¼‰
                    normalized_title = article.title.lower().strip() if article.title else ""
                    if normalized_title:
                        sent_article_titles.add(normalized_title)

        print(f"ğŸ” æ‰¾åˆ° {len(sent_article_titles)} ç¯‡ä¹‹å‰å¤„ç†è¿‡çš„è®ºæ–‡")
        return sent_article_titles

    except Exception as e:
        print(f"âŒ è·å–ä¹‹å‰å‘é€è®°å½•å¤±è´¥: {e}")
        return set()


async def _batch_send_emails_task(
    search_id: int,
    subject: str,
    include_author_emails: bool,
    include_pdf_emails: bool,
    db: AsyncSession
):
    """åå°ä»»åŠ¡ï¼šæ‰¹é‡å‘é€é‚®ä»¶"""
    try:
        # é‡æ–°è·å–æœç´¢è®°å½•å’Œæ–‡ç« 
        result = await db.execute(
            select(SearchDB)
            .options(selectinload(SearchDB.articles))
            .where(SearchDB.id == search_id)
        )
        search = result.scalar_one_or_none()

        if not search or not search.articles:
            return

        # è·å–ä¹‹å‰å·²ç»å‘é€è¿‡é‚®ä»¶çš„è®ºæ–‡æ ‡é¢˜
        previously_sent_titles = await _get_previously_sent_articles(search_id, db)

        sender = get_email_sender()

        # åˆ›å»ºä¸€ä¸ªé›†åˆæ¥è·Ÿè¸ªå·²ç»å‘é€çš„é‚®ç®±åœ°å€
        sent_emails = set()

        # ç»Ÿè®¡æ€»é‚®ç®±æ•°ï¼ˆæ’é™¤é‡å¤è®ºæ–‡å’Œé‡å¤é‚®ç®±ï¼‰
        total_emails = 0
        skipped_articles = 0
        for article in search.articles:
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²ç»å‘é€è¿‡é‚®ä»¶
            normalized_title = article.title.lower().strip() if article.title else ""
            if normalized_title in previously_sent_titles:
                skipped_articles += 1
                print(f"â­ï¸ è·³è¿‡é‡å¤è®ºæ–‡: {article.title[:50]}...")
                continue

            # ç»Ÿè®¡ä½œè€…é‚®ç®±ï¼ˆå»é‡ï¼‰
            if include_author_emails and article.author_emails:
                for author_email in article.author_emails:
                    if author_email.get('email') and author_email['email'] not in sent_emails:
                        total_emails += 1

            # ç»Ÿè®¡PDFé‚®ç®±ï¼ˆå»é‡ï¼‰
            if include_pdf_emails and article.pdf_fallback_emails:
                for pdf_email in article.pdf_fallback_emails:
                    if pdf_email not in sent_emails:
                        total_emails += 1

        print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ: æ€»è®ºæ–‡ {len(search.articles)} ç¯‡ï¼Œè·³è¿‡é‡å¤ {skipped_articles} ç¯‡ï¼Œå¾…å‘é€é‚®ç®± {total_emails} ä¸ª")

        sent_count = 0
        failed_count = 0

        # å‘é€è¿›åº¦æ›´æ–°
        await send_progress_update(f"batch_email_{search_id}", {
            "type": "progress",
            "step": "batch_sending",
            "title": "æ‰¹é‡å‘é€é‚®ä»¶",
            "description": f"å¼€å§‹å‘é€ {total_emails} å°é‚®ä»¶",
            "progress": 0,
            "total": total_emails,
            "sent": 0,
            "failed": 0
        })

        for article in search.articles:
            # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²ç»å‘é€è¿‡é‚®ä»¶
            normalized_title = article.title.lower().strip() if article.title else ""
            if normalized_title in previously_sent_titles:
                print(f"â­ï¸ è·³è¿‡é‡å¤è®ºæ–‡: {article.title[:50]}...")
                continue

            # å‘é€ä½œè€…é‚®ç®±
            if include_author_emails and article.author_emails:
                for author_email in article.author_emails:
                    if author_email.get('email') and author_email['email'] not in sent_emails:
                        try:
                            template_data = {
                                'author_name': author_email.get('name', 'Fellow Researcher'),
                                'paper_title': article.title,
                                'paper_venue': article.venue,
                                'paper_year': article.year,
                                'paper_citations': article.citations
                            }

                            result = sender.send_email(
                                to_email=author_email['email'],
                                subject=subject,
                                template_data=template_data
                            )

                            if result.get('success'):
                                sent_count += 1
                                # å°†é‚®ç®±æ·»åŠ åˆ°å·²å‘é€é›†åˆä¸­
                                sent_emails.add(author_email['email'])
                                print(f"âœ… æˆåŠŸå‘é€é‚®ä»¶åˆ° {author_email['email']}")
                            else:
                                failed_count += 1
                                print(f"âŒ å‘é€é‚®ä»¶åˆ° {author_email['email']} å¤±è´¥: {result.get('message')}")

                        except Exception as e:
                            failed_count += 1
                            print(f"âŒ å‘é€é‚®ä»¶åˆ° {author_email['email']} å¼‚å¸¸: {e}")

                        # æ›´æ–°è¿›åº¦
                        progress = int((sent_count + failed_count) / total_emails * 100)
                        await send_progress_update(f"batch_email_{search_id}", {
                            "type": "progress",
                            "step": "batch_sending",
                            "title": "æ‰¹é‡å‘é€é‚®ä»¶",
                            "description": f"å·²å‘é€ {sent_count}/{total_emails} å°é‚®ä»¶",
                            "progress": progress,
                            "total": total_emails,
                            "sent": sent_count,
                            "failed": failed_count
                        })

                        # æ·»åŠ å»¶è¿Ÿé¿å…å‘é€è¿‡å¿«
                        await asyncio.sleep(5)

            # å‘é€PDFé‚®ç®±
            if include_pdf_emails and article.pdf_fallback_emails:
                for pdf_email in article.pdf_fallback_emails:
                    if pdf_email not in sent_emails:
                        try:
                            template_data = {
                                'author_name': 'Fellow Researcher',
                                'paper_title': article.title,
                                'paper_venue': article.venue,
                                'paper_year': article.year,
                                'paper_citations': article.citations
                            }

                            result = sender.send_email(
                                to_email=pdf_email,
                                subject=subject,
                                template_data=template_data
                            )

                            if result.get('success'):
                                sent_count += 1
                                # å°†é‚®ç®±æ·»åŠ åˆ°å·²å‘é€é›†åˆä¸­
                                sent_emails.add(pdf_email)
                                print(f"âœ… æˆåŠŸå‘é€é‚®ä»¶åˆ° {pdf_email}")
                            else:
                                failed_count += 1
                                print(f"âŒ å‘é€é‚®ä»¶åˆ° {pdf_email} å¤±è´¥: {result.get('message')}")

                        except Exception as e:
                            failed_count += 1
                            print(f"âŒ å‘é€é‚®ä»¶åˆ° {pdf_email} å¼‚å¸¸: {e}")

                        # æ›´æ–°è¿›åº¦
                        progress = int((sent_count + failed_count) / total_emails * 100)
                        await send_progress_update(f"batch_email_{search_id}", {
                            "type": "progress",
                            "step": "batch_sending",
                            "title": "æ‰¹é‡å‘é€é‚®ä»¶",
                            "description": f"å·²å‘é€ {sent_count}/{total_emails} å°é‚®ä»¶",
                            "progress": progress,
                            "total": total_emails,
                            "sent": sent_count,
                            "failed": failed_count
                        })

                        # æ·»åŠ å»¶è¿Ÿé¿å…å‘é€è¿‡å¿«
                        await asyncio.sleep(5)

        # å‘é€å®Œæˆæ¶ˆæ¯
        await send_progress_update(f"batch_email_{search_id}", {
            "type": "completion",
            "step": "batch_complete",
            "title": "æ‰¹é‡å‘é€å®Œæˆ",
            "description": f"æˆåŠŸå‘é€ {sent_count} å°é‚®ä»¶ï¼Œå¤±è´¥ {failed_count} å°",
            "status": "completed",
            "result": {
                "total": total_emails,
                "sent": sent_count,
                "failed": failed_count
            }
        })

        print(f"ğŸ‰ æ‰¹é‡å‘é€å®Œæˆ: æˆåŠŸ {sent_count}/{total_emails} å°é‚®ä»¶")

    except Exception as e:
        print(f"âŒ æ‰¹é‡å‘é€é‚®ä»¶å¤±è´¥: {e}")
        await send_progress_update(f"batch_email_{search_id}", {
            "type": "completion",
            "step": "batch_failed",
            "title": "æ‰¹é‡å‘é€å¤±è´¥",
            "description": f"æ‰¹é‡å‘é€é‚®ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "status": "failed"
        })


